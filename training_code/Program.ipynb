{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program Hyperparameters\n",
    "\n",
    "neighborhood_size = 2\n",
    "embedding_dims = 2\n",
    "negative = 5\n",
    "device = 'cuda'\n",
    "num_epochs = 200\n",
    "learning_rate = 2e-1\n",
    "lr_decay = 0.99\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import decomposition\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "\n",
    "with open('walks') as f:\n",
    "    corpus = f.read().split('\\n')\n",
    "\n",
    "corpus = sample(corpus, len(corpus) // 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64664\n"
     ]
    }
   ],
   "source": [
    "print(sum(map(lambda x: len(x.split(' ')), corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def create_vocabulary(corpus):\n",
    "    '''Creates a dictionary with all unique words in corpus with id'''\n",
    "    vocabulary = {}\n",
    "    i = 0\n",
    "    for s in corpus:\n",
    "        for w in s.split():\n",
    "            if w not in vocabulary:\n",
    "                vocabulary[w] = i\n",
    "                i+=1\n",
    "    return vocabulary\n",
    "\n",
    "def prepare_set(corpus, n_gram = 1):\n",
    "    '''Creates a dataset with Input column and Outputs columns for neighboring words. \n",
    "       The number of neighbors = n_gram*2'''\n",
    "    columns = ['Input'] + [f'Output{i+1}' for i in range(n_gram*2)]\n",
    "    result = pd.DataFrame(columns = columns)\n",
    "    for sentence in corpus:\n",
    "        for i,w in enumerate(sentence.split()):\n",
    "            inp = [w]\n",
    "            out = []\n",
    "            for n in range(1,n_gram+1):\n",
    "                # look back\n",
    "                if (i-n)>=0:\n",
    "                    out.append(sentence.split()[i-n])\n",
    "                else:\n",
    "                    out.append('<padding>')\n",
    "                \n",
    "                # look forward\n",
    "                if (i+n)<len(sentence.split()):\n",
    "                    out.append(sentence.split()[i+n])\n",
    "                else:\n",
    "                    out.append('<padding>')\n",
    "            row = pd.DataFrame([inp+out], columns = columns)\n",
    "            result = result.append(row, ignore_index = True)\n",
    "    return result\n",
    "\n",
    "def prepare_set_ravel(corpus, n_gram = 1):\n",
    "    '''Creates a dataset with Input column and Output column for neighboring words. \n",
    "       The number of neighbors = n_gram*2'''\n",
    "    columns = ['Input', 'Output']\n",
    "    result = pd.DataFrame(columns = columns)\n",
    "    k = 0\n",
    "    o = widgets.HTML()\n",
    "    display(o)\n",
    "    for sentence in corpus:\n",
    "        o.value = (f'<p>{k / len(corpus)}% ({k} / {len(corpus)})</p>')\n",
    "        k += 1\n",
    "        for i,w in enumerate(sentence.split()):\n",
    "            inp = w\n",
    "            for n in range(1,n_gram+1):\n",
    "                # look back\n",
    "                if (i-n)>=0:\n",
    "                    out = sentence.split()[i-n]\n",
    "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
    "                    result = result.append(row, ignore_index = True)\n",
    "                \n",
    "                # look forward\n",
    "                if (i+n)<len(sentence.split()):\n",
    "                    out = sentence.split()[i+n]\n",
    "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
    "                    result = result.append(row, ignore_index = True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4205a9f261294d30af680232b0fdadb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Vocabulary for indexing\n",
    "vocabulary = create_vocabulary(corpus)\n",
    "\n",
    "print(\"Vocab done\")\n",
    "\n",
    "# Create train embedding\n",
    "# train_emb = prepare_set(corpus, n_gram=neighborhood_size)\n",
    "train_emb = prepare_set_ravel(corpus, n_gram=neighborhood_size)\n",
    "train_emb.Input = train_emb.Input.map(vocabulary)\n",
    "train_emb.Output = train_emb.Output.map(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Skip-Gram model described in paper:\n",
    "    https://arxiv.org/abs/1301.3781\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SkipGram, self).__init__()\n",
    "        vocab_size = len(vocabulary)\n",
    "        \n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dims\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_dims,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 10.742963790893555\n",
      "Epoch 50, loss = 9.851836204528809\n",
      "Epoch 100, loss = 9.75027084350586\n",
      "Epoch 150, loss = 9.688007354736328\n",
      "CPU times: user 7min 8s, sys: 504 ms, total: 7min 9s\n",
      "Wall time: 7min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "        \n",
    "# define loss func\n",
    "model = SkipGram()\n",
    "model = model.to(device)\n",
    "\n",
    "loss_f = torch.nn.CrossEntropyLoss() # see details: https://pytorch.org/docs/stable/nn.html\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "input_loader = DataLoader(train_emb.Input.values, batch_size=1024)\n",
    "label_loader = DataLoader(train_emb.Output.values, batch_size=1024)\n",
    "\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    for x,y in zip(input_loader, label_loader):\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "     \n",
    "        y_pred = model(x)\n",
    "\n",
    "        #compute loss\n",
    "        loss = loss_f(y_pred, y)\n",
    "        \n",
    "        # bakpropagation step\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    if epo%10 == 0:\n",
    "        learning_rate *= lr_decay\n",
    "    loss_hist.append(loss)\n",
    "    if epo%50 == 0:\n",
    "        print(f'Epoch {epo}, loss = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results.csv\")\n",
    "df.index = df['_id']\n",
    "df = df['_labels'].apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f8f80a66b180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-f8f80a66b180>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "list(map(lambda x: model.embeddings(torch.tensor(x, device=device)), list(df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loss_f = torch.nn.CrossEntropyLoss() # see details: https://pytorch.org/docs/stable/nn.html\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "input_loader = DataLoader(df.index, batch_size=1024)\n",
    "label_loader = DataLoader(df, batch_size=1024)\n",
    "\n",
    "convert = {\n",
    "    'Author': torch.tensor(0,device = device),\n",
    "    'Paper': torch.tensor(1, device=device),\n",
    "    'Conference': torch.tensor(2, device=device)\n",
    "}\n",
    "\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    for x,y in zip(input_loader, label_loader):\n",
    "        \n",
    "        x = torch.tensor(x, device=device)\n",
    "        y = torch.cat(list(map(lambda z: convert[z], y)))\n",
    "        \n",
    "        optim.zero_grad()\n",
    "     \n",
    "        y_pred = model.embeddings(x)\n",
    "\n",
    "        #compute loss\n",
    "        loss = loss_f(y_pred, y)\n",
    "        \n",
    "        # bakpropagation step\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    if epo%10 == 0:\n",
    "        learning_rate *= lr_decay\n",
    "    loss_hist.append(loss)\n",
    "    if epo%50 == 0:\n",
    "        print(f'Epoch {epo}, loss = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
