{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program Hyperparameters\n",
    "\n",
    "neighborhood_size = 2\n",
    "embedding_dims = 2\n",
    "negative = 5\n",
    "device = 'cuda'\n",
    "num_epochs = 200\n",
    "learning_rate = 2e-1\n",
    "lr_decay = 0.99\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import decomposition\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "\n",
    "with open('walks') as f:\n",
    "    corpus = f.read().split('\\n')\n",
    "\n",
    "corpus = sample(corpus, len(corpus) // 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def create_vocabulary(corpus):\n",
    "    '''Creates a dictionary with all unique words in corpus with id'''\n",
    "    vocabulary = {}\n",
    "    i = 0\n",
    "    for s in corpus:\n",
    "        for w in s.split():\n",
    "            if w not in vocabulary:\n",
    "                vocabulary[w] = i\n",
    "                i+=1\n",
    "    return vocabulary\n",
    "\n",
    "def prepare_set(corpus, n_gram = 1):\n",
    "    '''Creates a dataset with Input column and Outputs columns for neighboring words. \n",
    "       The number of neighbors = n_gram*2'''\n",
    "    columns = ['Input'] + [f'Output{i+1}' for i in range(n_gram*2)]\n",
    "    result = pd.DataFrame(columns = columns)\n",
    "    for sentence in corpus:\n",
    "        for i,w in enumerate(sentence.split()):\n",
    "            inp = [w]\n",
    "            out = []\n",
    "            for n in range(1,n_gram+1):\n",
    "                # look back\n",
    "                if (i-n)>=0:\n",
    "                    out.append(sentence.split()[i-n])\n",
    "                else:\n",
    "                    out.append('<padding>')\n",
    "                \n",
    "                # look forward\n",
    "                if (i+n)<len(sentence.split()):\n",
    "                    out.append(sentence.split()[i+n])\n",
    "                else:\n",
    "                    out.append('<padding>')\n",
    "            row = pd.DataFrame([inp+out], columns = columns)\n",
    "            result = result.append(row, ignore_index = True)\n",
    "    return result\n",
    "\n",
    "def prepare_set_ravel(corpus, n_gram = 1):\n",
    "    '''Creates a dataset with Input column and Output column for neighboring words. \n",
    "       The number of neighbors = n_gram*2'''\n",
    "    columns = ['Input', 'Output']\n",
    "    result = pd.DataFrame(columns = columns)\n",
    "    k = 0\n",
    "    o = widgets.HTML()\n",
    "    display(o)\n",
    "    for sentence in corpus:\n",
    "        o.value = (f'<p>{k / len(corpus)}% ({k} / {len(corpus)})</p>')\n",
    "        k += 1\n",
    "        for i,w in enumerate(sentence.split()):\n",
    "            inp = w\n",
    "            for n in range(1,n_gram+1):\n",
    "                # look back\n",
    "                if (i-n)>=0:\n",
    "                    out = sentence.split()[i-n]\n",
    "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
    "                    result = result.append(row, ignore_index = True)\n",
    "                \n",
    "                # look forward\n",
    "                if (i+n)<len(sentence.split()):\n",
    "                    out = sentence.split()[i+n]\n",
    "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
    "                    result = result.append(row, ignore_index = True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55aff93f13942a6a07ab43073225ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Vocabulary for indexing\n",
    "vocabulary = create_vocabulary(corpus)\n",
    "\n",
    "print(\"Vocab done\")\n",
    "\n",
    "# Create train embedding\n",
    "# train_emb = prepare_set(corpus, n_gram=neighborhood_size)\n",
    "train_emb = prepare_set_ravel(corpus, n_gram=neighborhood_size)\n",
    "train_emb.Input = train_emb.Input.map(vocabulary)\n",
    "train_emb.Output = train_emb.Output.map(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Skip-Gram model described in paper:\n",
    "    https://arxiv.org/abs/1301.3781\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SkipGram, self).__init__()\n",
    "        vocab_size = len(vocabulary)\n",
    "        \n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dims\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_dims,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "def print_size(model):    \n",
    "    param_size = 0\n",
    "    buffer_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    print((param_size + buffer_size) / (1024 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([246768])\n",
      "torch.Size([246768, 1])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 35.20 GiB (GPU 0; 7.93 GiB total capacity; 11.67 MiB already allocated; 6.61 GiB free; 26.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-ae4e6efc16be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-8478a1bb22fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 35.20 GiB (GPU 0; 7.93 GiB total capacity; 11.67 MiB already allocated; 6.61 GiB free; 26.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# define loss func\n",
    "model = SkipGram()\n",
    "model = model.to(device)\n",
    "\n",
    "loss_f = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "inputs_dataloader = DataLoader(train_emb.Input.values, batch_size=train_emb.shape[0])\n",
    "label_dataloader = DataLoader(train_emb.Output.values, batch_size=train_emb.shape[0])\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    for x,y in zip(inputs_dataloader, label_dataloader):\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        y_pred = model(x)\n",
    "\n",
    "        #compute loss\n",
    "        loss = loss_f(y_pred, y)\n",
    "        \n",
    "        # bakpropagation step\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    if epo % 10 == 0:\n",
    "        learning_rate *= lr_decay\n",
    "    loss_hist.append(loss)\n",
    "    if epo % 50 == 0:\n",
    "        print(f'Epoch {epo}, loss = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
